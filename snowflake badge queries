snowflake.sql

https://data-engineering-simplified.medium.com/flatten-function-in-snowflake-29b9ee54124f

-- Snowflake Queries..


-- CHEMA USAGE :
use database UTIL_DB;
use schema PUBLIC;
use role ACCOUNTADMIN;
------------------------

-- CREATING VIEW :
create view intl_db.public.NATIONS_SAMPLE_PLUS_ISO ( iso_country_name
  ,country_name_official
  ,alpha_code_2digit
  ,region) AS 
  select  
     iso_country_name
    ,country_name_official,alpha_code_2digit
    ,r_name as region
from INTL_DB.PUBLIC.INT_STDS_ORG_3166 i
left join SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.NATION n
on upper(i.iso_country_name)= n.n_name
left join SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.REGION r
on n_regionkey = r_regionkey;

---------------------------------------------
list  @util_db.public.like_a_window_into_an_s3_bucket;

-- LIBRARY_CARD_CATALOG.PUBLIC.JSON_FILE_FORMAT 

COPY INTO AUTHOR_INGEST_JSON 
FROM @util_db.public.like_a_window_into_an_s3_bucket
FILES = ('author_with_header.json')
FILE_FORMAT = (format_name = LIBRARY_CARD_CATALOG.PUBLIC.JSON_FILE_FORMAT);

    CREATE FILE FORMAT LIBRARY_CARD_CATALOG.PUBLIC.JSON_FILE_FORMAT 
    TYPE = 'JSON' 
    COMPRESSION = 'AUTO' 
    ENABLE_OCTAL = FALSE
    ALLOW_DUPLICATE = FALSE 
    STRIP_OUTER_ARRAY = TRUE
    STRIP_NULL_VALUES = FALSE 
    IGNORE_UTF8_ERRORS = FALSE ;


    select * from AUTHOR_INGEST_JSON ;
    truncate table AUTHOR_INGEST_JSON ;

    drop file format JSON_FILE_FORMAT;
    truncate table author_ingest_json;

    show FILE FORMATS;
    
    select raw_author:AUTHOR_NAME
    from author_ingest_json
    where raw_author:AUTHOR_UID = '1';

    select raw_author
    from author_ingest_json;
    
    SELECT 
    raw_author:AUTHOR_UID
    ,raw_author:FIRST_NAME::STRING as FIRST_NAME
    ,raw_author:MIDDLE_NAME::STRING as MIDDLE_NAME
    ,raw_author:LAST_NAME::STRING as LAST_NAME
    FROM AUTHOR_INGEST_JSON;


    select GRADER(step, (actual = expected), actual, expected, description) as graded_results from
(
  SELECT 'DWW16' as step
  ,( select row_count 
    from LIBRARY_CARD_CATALOG.INFORMATION_SCHEMA.TABLES 
    where table_name = 'AUTHOR_INGEST_JSON') as actual
  ,6 as expected
  ,'Check number of rows' as description
 ); 

-------------------------------------------------


create view simple_currency (CTY_CODE,CUR_CODE)
as 
select COUNTRY_CHAR_CODE,CURRENCY_CHAR_CODE from COUNTRY_CODE_TO_CURRENCY_CODE
;
-------------------------------------
-- CREATING A  TABLE :
create table intl_db.public.CURRENCIES 
(
  currency_ID integer, 
  currency_char_code varchar(3), 
  currency_symbol varchar(4), 
  currency_digital_code varchar(3), 
  currency_digital_name varchar(30)
)
  comment = 'Information about currencies including character codes, symbols, digital codes, etc.';
-----------------------------------

-- FILE FORMAT CREATION :
create file format util_db.public.CSV_COMMA_LF_HEADER
  type = 'CSV' 
  field_delimiter = ',' 
  record_delimiter = '\n' -- the n represents a Line Feed character
  skip_header = 1 ;
;

---------------------------------
copy into my_table_name
from @like_a_window_into_an_s3_bucket
files = ( 'IF_I_HAD_A_FILE_LIKE_THIS.txt')
file_format = ( format_name='EXAMPLE_FILEFORMAT' );

list @UTIL_DB.PUBLIC.AWS_S3_BUCKET/c;

-- To load into CURRENCIES table
copy into intl_db.public.CURRENCIES 
from @AWS_S3_BUCKET
files = ( 'currencies.csv')
file_format = ( format_name='CSV_COMMA_LF_HEADER');


select count(*) from intl_db.public.CURRENCIES ;

-- to load into currency code
copy into intl_db.public.COUNTRY_CODE_TO_CURRENCY_CODE 
from @AWS_S3_BUCKET    --s3://uni-cmcw/country_code_to_currency_code.csv
files = ( 'country_code_to_currency_code.csv')
file_format = ( format_name='CSV_COMMA_LF_HEADER');

-------------------------------------

COPY INTO "SMOOTHIES"."PUBLIC"."FRUIT_OPTIONS"
FROM '@"SMOOTHIES"."PUBLIC"."%FRUIT_OPTIONS"/__snowflake_temp_import_files__/'
FILES = ('fruits_available_for_smoothies.txt')
FILE_FORMAT = (
    TYPE=CSV,
    SKIP_HEADER=2,
    FIELD_DELIMITER='%',
    TRIM_SPACE=FALSE,
    FIELD_OPTIONALLY_ENCLOSED_BY=NONE,
    REPLACE_INVALID_CHARACTERS=TRUE,
    DATE_FORMAT=AUTO,
    TIME_FORMAT=AUTO,
    TIMESTAMP_FORMAT=AUTO
)
ON_ERROR=ABORT_STATEMENT
PURGE=TRUE;

--Creating a file format :
CREATE FILE FORMAT SMOOTHIES.PUBLIC.TWO_GEADERROW_PCT_DELIM
TYPE=CSV,
    SKIP_HEADER=2,
    FIELD_DELIMITER='%',
    TRIM_SPACE=FALSE,
    FIELD_OPTIONALLY_ENCLOSED_BY=NONE,
    REPLACE_INVALID_CHARACTERS=TRUE,
    DATE_FORMAT=AUTO,
    TIME_FORMAT=AUTO,
    TIMESTAMP_FORMAT=AUTO;
--------------
-- CREATE TABLE ::
create or replace TABLE SMOOTHIES.PUBLIC.FRUIT_OPTIONS (
	FRUIT_ID NUMBER(38,0),
	FRUIT_NAME VARCHAR(100)
);

-------------------


-- COPY INTO Statement ;
COPY INTO "SMOOTHIES"."PUBLIC"."FRUIT_OPTIONS"
FROM '@"SMOOTHIES"."PUBLIC"."%FRUIT_OPTIONS"/__snowflake_temp_import_files__/'
FILES = ('fruits_available_for_smoothies.txt')
FILE_FORMAT = (FORMAT_NAME = SMOOTHIES.PUBLIC.TWO_GEADERROW_PCT_DELIM)
ON_ERROR=ABORT_STATEMENT
PURGE=TRUE;

---------------------
-- STAGING, COPYING INTO INTERNAL STAGE

COPY INTO "SMOOTHIES"."PUBLIC"."FRUIT_OPTIONS"
FROM @smoothies.public.my_internal_stage
FILES = ('fruits_available_for_smoothies.txt')
FILE_FORMAT = (FORMAT_NAME = SMOOTHIES.PUBLIC.TWO_HEADERROW_PCT_DELIM)
ON_ERROR=ABORT_STATEMENT
VALIDATION_MODE = RETURN_ERRORS
PURGE=TRUE;

-- ACCESSING THE STAGE, VIEWING FORMAT FROM AN INTERNAL STAGE
SELECT $1, $2 
FROM @smoothies.public.my_internal_stage/fruits_available_for_smoothies.txt
(FILE_FORMAT => smoothies.public.TWO_HEADERROW_PCT_DELIM);
----------------------------------------
-- https://docs.snowflake.com/en/user-guide/data-load-transform#reorder-csv-columns-during-a-load
-- > COPY INTO TABLE, USING THE $1 ATRIBUTE METHODS...
----------------------------------------


-- COPY INTO USING $1, $2 PARAMETERS FROMSTAGE...

COPY INTO "SMOOTHIES"."PUBLIC"."FRUIT_OPTIONS"
FROM (SELECT $2 AS FRUIT_ID, $1 AS FRUIT_NAME
FROM @smoothies.public.my_internal_stage/fruits_available_for_smoothies.tx)
FILE_FORMAT = (FORMAT_NAME = SMOOTHIES.PUBLIC.TWO_HEADERROW_PCT_DELIM)
ON_ERROR=ABORT_STATEMENT
PURGE=TRUE;

----------------------------------------
-- creating a sequence :

create sequence order_seq
    start = 1
    increment = 1
    comment = 'sequecne generator'


alter table SMOOTHIES.PUBLIC.ORDERS 
add column order_uid integer --adds the column
default smoothies.public.order_seq.nextval  --sets the value of the column to sequence
constraint order_uid unique enforced;

----------------------------------------
-- CREATING A TABLE WITH SEQ NUMBER
create or replace TABLE SMOOTHIES.PUBLIC.ORDERS (
	INGREDIENTS VARCHAR(200),
	NAME_ON_ORDER VARCHAR(100),
	ORDER_FILLED BOOLEAN DEFAULT FALSE,
	ORDER_UID NUMBER(38,0) DEFAULT SMOOTHIES.PUBLIC.ORDER_SEQ.NEXTVAL,
	constraint ORDER_UID unique (ORDER_UID)
);

----------------------------------------

create or replace table smoothies.public.orders (
       order_uid number(38,0) default smoothies.public.order_seq.nextval,
       order_filled boolean default false,
       name_on_order varchar(100),
       ingredients varchar(200),
       order_ts timestamp_ltz(9) default current_timestamp(),
       constraint order_uid unique (order_uid)
);
----------------------------------------
--create and alter :

  alter table SMOOTHIES.PUBLIC.ORDERS
    add column ORDER_FILLED BOOLEAN DEFAULT FALSE ;

    select * from SMOOTHIES.PUBLIC.ORDERS;

    truncate table SMOOTHIES.PUBLIC.ORDERS;

    update smoothies.public.orders
       set order_filled = true
       where name_on_order is null;
----------------------------------------

-- USER DEFINED FUNCTIONS :
CREATE OR REPLACE FUNCTION util_db.public.sum_mystery_bag_vars(
    "this" NUMBER(38,0),
    "that" NUMBER(38,0),
    "the_other" NUMBER(38,0))
RETURNS NUMBER(38,0)
LANGUAGE SQL
AS 'select this+that+the_other';

select alternating_caps_phrase
set  alternating_caps_phrase = 'bUt mOm i wAsHeD tHe dIsHes yEsTeRdAy'
select initcap($alternating_caps_phrase);

CREATE OR REPLACE FUNCTION util_db.public.NEUTRALIZE_WHINING
  ( "ABC" TEXT )
RETURNS TEXT
LANGUAGE SQL
as 'select INITCAP($alternating_caps_phrase)'

select function util_db.public.NEUTRALIZE_WHINING

select * from alternating_caps_phrase;


select GRADER(step, (actual = expected), actual, expected, description) as graded_results from (
 SELECT 'DABW007' as step
 ,( select hash(neutralize_whining('bUt mOm i wAsHeD tHe dIsHes yEsTeRdAy'))) as actual
 , -4759027801154767056 as expected
 ,'WHINGE UDF Works' as description
);
    
    
----------------------------------------
--- Creating external Stage :  s3://uni-klaus/clothing
CREATE STAGE UNI_KLAUS_CLOTHING 
	URL = 's3://uni-klaus/clothing' 
	DIRECTORY = ( ENABLE = true );

  CREATE STAGE UNI_KLAUS_ZMD 
	URL = 's3://uni-klaus/zenas_metadata' 
	DIRECTORY = ( ENABLE = true );

  CREATE STAGE UNI_KLAUS_SNEAKERS 
	URL = 's3://uni-klaus/sneakers' 
	DIRECTORY = ( ENABLE = true );

  list @ZENAS_ATHLEISURE_DB;

  -- selecting a column from a stage file :
  select $1
  from @uni_klaus_zmd/product_coordination_suggestions.txt; 

  create file format zmd_file_format_1
  RECORD_DELIMITER = '^';


  -- CRLF (Carriage Return Line Feed) as the record delimiter to remove extra spaces :

select REPLACE($1,chr(13)||chr(10)) as sizes_available
from @uni_klaus_zmd/sweatsuit_sizes
(file_format => zmd_file_format_2 );


-- View Creation :

create view zenas_athleisure_db.products.sweatsuit_sizes as (
select REPLACE($1,chr(13)||chr(10)) as sizes_available
from @uni_klaus_zmd/sweatsuit_sizes
(file_format => zmd_file_format_2 )
where sizes_available <> '');

create view zenas_athleisure_db.products.SWEATBAND_PRODUCT_LINE as (
select REPLACE($1,chr(13)||chr(10)) as PRODUCT_CODE, REPLACE($2,chr(13)||chr(10)) as HEADBAND_DESCRIPTION,REPLACE($3,chr(13)||chr(10)) as WRISTBAND_DESCRIPTION
from @uni_klaus_zmd/swt_product_line.txt
(file_format => zmd_file_format_2 )
)
;

create view zenas_athleisure_db.products.SWEATBAND_COORDINATION as 
(
select  REPLACE($1,chr(13)||chr(10)) as PRODUCT_CODE , REPLACE($2,chr(13)||chr(10)) as HAS_MATCHING_SWEATSUIT
from @uni_klaus_zmd/product_coordination_suggestions.txt
(file_format => zmd_file_format_3)
);

-- ZENAS_ATHLEISURE_DB  _> for UNI_KLAUS_CLOTHING
-- Meta data .png files data from a stage object...
select metadata$filename, count(metadata$file_row_number)
from @uni_klaus_clothing
group by metadata$filename;

---  CSV, JSON, XML, PARQUET, ORC, & AVRO
-- * Unstruccyured data support...( audio, .png etc)

-- Directory Tables --> To handle unstructured data....
-- They are attached to a Stage (internal or external).  
-- You have to enable them. 
-- You have to refresh them. 


select * from directory(@uni_klaus_clothing);

select * from directory(@uni_klaus_clothing);

alter stage uni_klaus_clothing 
set directory = (enable = true);


alter stage uni_klaus_clothing refresh;


select UPPER(RELATIVE_PATH) as uppercase_filename
, REPLACE(uppercase_filename,'/') as no_slash_filename
, REPLACE(no_slash_filename,'_',' ') as no_underscores_filename
, REPLACE(no_underscores_filename,'.PNG') as just_words_filename
from directory(@uni_klaus_clothing);

select 
 REPLACE(REPLACE(REPLACE(REPLACE(UPPER(RELATIVE_PATH),'/'),'_',' '),'.PNG'),'SWEATSUIT','') as PRODUCT_NAME
-- , REPLACE(no_slash_filename,'_') as no_underscores_filename
-- , REPLACE(no_underscores_filename,'.PNG') as product_name
from directory(@uni_klaus_clothing);

create or replace TABLE ZENAS_ATHLEISURE_DB.PRODUCTS.SWEATSUITS (
	COLOR_OR_STYLE VARCHAR(25),
	DIRECT_URL VARCHAR(200),
	PRICE NUMBER(5,2)
);


insert into  ZENAS_ATHLEISURE_DB.PRODUCTS.SWEATSUITS 
          (COLOR_OR_STYLE, DIRECT_URL, PRICE)
values
('90s', 'https://uni-klaus.s3.us-west-2.amazonaws.com/clothing/90s_tracksuit.png',500)
,('Burgundy', 'https://uni-klaus.s3.us-west-2.amazonaws.com/clothing/burgundy_sweatsuit.png',65)
,('Charcoal Grey', 'https://uni-klaus.s3.us-west-2.amazonaws.com/clothing/charcoal_grey_sweatsuit.png',65)
,('Forest Green', 'https://uni-klaus.s3.us-west-2.amazonaws.com/clothing/forest_green_sweatsuit.png',65)
,('Navy Blue', 'https://uni-klaus.s3.us-west-2.amazonaws.com/clothing/navy_blue_sweatsuit.png',65)
,('Orange', 'https://uni-klaus.s3.us-west-2.amazonaws.com/clothing/orange_sweatsuit.png',65)
,('Pink', 'https://uni-klaus.s3.us-west-2.amazonaws.com/clothing/pink_sweatsuit.png',65)
,('Purple', 'https://uni-klaus.s3.us-west-2.amazonaws.com/clothing/purple_sweatsuit.png',65)
,('Red', 'https://uni-klaus.s3.us-west-2.amazonaws.com/clothing/red_sweatsuit.png',65)
,('Royal Blue',	'https://uni-klaus.s3.us-west-2.amazonaws.com/clothing/royal_blue_sweatsuit.png',65)
,('Yellow', 'https://uni-klaus.s3.us-west-2.amazonaws.com/clothing/yellow_sweatsuit.png',65);*9


select color_or_style
, direct_url
, price
, size as image_size
, last_modified as image_last_modified
from sweatsuits s
join directory(@uni_klaus_clothing) d
on  d.relative_path = replace(s.direct_url, 'https://uni-klaus.s3.us-west-2.amazonaws.com/clothing');



-- Cartision Product :
select color_or_style
, direct_url
, price
, size as image_size
, last_modified as image_last_modified
from sweatsuits s
join directory(@uni_klaus_clothing) d
on  d.relative_path = replace(s.direct_url, 'https://uni-klaus.s3.us-west-2.amazonaws.com/clothing');


-------------streamlit app deploy :
# .streamlit/secrets.toml

[connections.snowflake]
account = "MGYKXFX-TG99297"
user = "VinayakDani"
password = "Mahadev%2912"
role = "SYSADMIN"
warehouse = "COMPUTE_WH"
database = "SMOOTHIES"
schema = "PUBLIC"
client_session_keep_alive = true

------------------------------


---- Creatng View, from External stage need not necessorly load the data, can create stage objects and can join with internal stage and unstructurred data 
--- can also be handled well with directory tables....from which can be accessed .png and unstructured data...

-- STAGE CREATE

CREATE STAGE TRAILS_PARQUET 
	URL = 's3://uni-lab-files-more bucket/dlkw/trails_parquet' 
	DIRECTORY = ( ENABLE = true );

  CREATE STAGE TRAILS_GEOJSON 
	URL = 's3://uni-lab-files-more/dlkw/trails_geojson' 
	DIRECTORY = ( ENABLE = true );

-- ## JSON File Format :

CREATE FILE FORMAT MELS_SMOOTHIE_CHALLENGE_DB.TRAILS.FF_JSON 
    TYPE = 'JSON' 
    COMPRESSION = 'AUTO' 
    ENABLE_OCTAL = FALSE
    ALLOW_DUPLICATE = FALSE 
    STRIP_OUTER_ARRAY = TRUE
    STRIP_NULL_VALUES = FALSE 
    IGNORE_UTF8_ERRORS = FALSE ;

-- ## PARQUET FILE FORMAT :
    CREATE FILE FORMAT MELS_SMOOTHIE_CHALLENGE_DB.TRAILS.FF_PARQUET 
    TYPE = 'PARQUET' 
    COMPRESSION = 'AUTO' ;



    select $1
    from @trails_parquet
    (file_format=> ff_parquet);

-- # selecting a JSON File format query from a file :
select 
 $1:sequence_1 as point_id,
 $1:trail_name::varchar as trail_name,
 $1:latitude::number(11,8) as lng, --remember we did a gut check on this data
 $1:longitude::number(11,8) as lat
from @trails_parquet
(file_format => ff_parquet)
order by point_id;


-- ## VIEW Creation :

create or replace view CHERRY_CREEK_TRAIL as 
(
select 
 $1:sequence_1 as point_id,
 $1:trail_name::varchar as trail_name,
 $1:latitude::number(11,8) as lng, --remember we did a gut check on this data
 $1:longitude::number(11,8) as lat
from @trails_parquet
(file_format => ff_parquet)
order by point_id
);


create or replace view cherry_creek_trail as
select 
 $1:sequence_1 as point_id,
 $1:trail_name::varchar as trail_name,
 $1:latitude::number(11,8) as lng,
 $1:longitude::number(11,8) as lat,
 lng||' '||lat as coord_pair
from @trails_parquet
(file_format => ff_parquet)
order by point_id;

-- > 
select 
'LINESTRING('||
listagg(coord_pair, ',') 
within group (order by point_id)
||')' as my_linestring
from cherry_creek_trail
where point_id <= 10
group by trail_name;


-- JSON DATA Normalising ::

select
$1:features[0]:properties:Name::string as feature_name
,$1:features[0]:geometry:coordinates::string as feature_coordinates
,$1:features[0]:geometry::string as geometry
,$1:features[0]:properties::string as feature_properties
,$1:crs:properties:name::string as specs
,$1 as whole_object
from @trails_geojson (file_format => ff_json);


create or replace view DENVER_AREA_TRAILS as(
select
$1:features[0]:properties:Name::string as feature_name
,$1:features[0]:geometry:coordinates::string as feature_coordinates
,$1:features[0]:geometry::string as geometry
,$1:features[0]:properties::string as feature_properties
,$1:crs:properties:name::string as specs
,$1 as whole_object
from @trails_geojson (file_format => ff_json));


select feature_name
, to_geography(geometry) as my_linestring
, st_xmin(my_linestring) as min_eastwest
, st_xmax(my_linestring) as max_eastwest
, st_ymin(my_linestring) as min_northsouth
, st_ymax(my_linestring) as max_northsouth
from DENVER_AREA_TRAILS
union all
select feature_name
, to_geography(geometry) as my_linestring
, st_xmin(my_linestring) as min_eastwest
, st_xmax(my_linestring) as max_eastwest
, st_ymin(my_linestring) as min_northsouth
, st_ymax(my_linestring) as max_northsouth
, trail_length
from DENVER_AREA_TRAILS_2;




create or replace view V_CHERRY_CREEK_TRAIL(
	POINT_ID,
	TRAIL_NAME,
	LNG,
	LAT,
	COORD_PAIR
) as
select 
 $1:sequence_1 as point_id,
 $1:trail_name::varchar as trail_name,
 $1:latitude::number(11,8) as lng,
 $1:longitude::number(11,8) as lat,
 lng||' '||lat as coord_pair
from @trails_parquet
(file_format => ff_parquet)
order by point_id;

SELECT /*+PARALLEL(4)*/ COUNT(*)
-- CLM_EVNT_KEY, SOR_CD, RCRD_CREATD_TMS, RCRD_CREATD_USER_ID
FROM EHUB_CLM_SDS.EHUB_CLM_EVNT
WHERE  EXTRACT(YEAR FROM RCRD_CREATD_TMS)  > 2020
AND SOR_CD ='822';;;;


-------------------------------------------------------------------------------------
----------- Snowflake Developer Guide -------------

Snowflake :
-  connecting to Snowflake and executing database commands.
-  Command line tool to connect to your Snowflake account.

snowsql -- Command line interface to interact with snowflake..

1. WebInterface
2. Command line
3. Connectors...


Snowflake fundamentals ;
1. Snowflake Warehouses  
	- ( virtual data warehouses to process the SQL statements that you execute.
	- Snowflake Warehouses overview....
	- Multi-cluster Warehouses
	- Warehouse Considerations 
	- Working with Warehouse.
	- Using the Query Acceleration Service.

2. Basics of Snowflake Tables and Views
	- Understanding Snowflake Table Structures
	- Table Design Considerations	
	- Overview of Views	
	- Working with Secure Views
	- Cloning Considerations
	- Table Storage Considerations

3. Basics of Data Types
	- Introduction to Snowflake Data Types
	- Numeric Data Types
	- String and Binary Data Types	
	- Logical Data Types	
	- Date & Time Data Types
	- Geospatial Data Types

*** Getting data in to Snowflake **   (Data load to sbowflake)
	- Understanding Data loading into snowflake..( Tasks, tools , techniques )
	- Bulk Loading ( COPY command to load the data directly on demand from S3 bucket, or other storage container into snowflake )
	- Snowpipe ( Load Data automatically as soon as it arrives)

* Views, Materialized Views, & Dynamic Tables
* Streams and tasks make executing complex task based solutions simple and easy.
  Streams allow you to track changes to database objects and tasks provide a mechanism to then execute SQL when those events occur...
* share data products privately, or use a direct share to quickly share data with someone in the same region.
* Listing -> Provide data and other information to other snowflake user,,, share data and access others shared...
* Sharing data from one snowflake account to anothe snowflake account....


? Snowflake Horizon
- Define and assign aggregation policies and projection policies to control what type of queries can be run against shared data.
  Aggregation policies require analysts to run queries that aggregate data rather than retrieving individual rows.
  Projection policies control whether an analyst can use a SELECT statement to project a particular column.

Snowflake DataHorizon helps  to do,,,,
View object insights [3] using a user interface that lets you learn information about tables and views without writing SQL. 
You can determine who is accessing the data, the queries that access the data most frequently, whether someone has been modifying 
the governance posture of the data, whether there are downstream or upstream dependencies on the data, and whether the data has been 
classified as sensitive.

??? 
How to connect to snowflake : Interact with Snowflake DB
- Snowsight, SnowSQL(Snowflake command-line client) and the Snowflake Classic Console., Snowflake Extension for Visual Studio Code
- Snowflake-provided clients, including SnowSQL (command line interface), connectors for Python and Spark, and drivers for Node.js,
  JDBC, ODBC, and more.

???
 Snowflake has partenered to connect with other technogy connectivity like datadog through JDBC , ODBC connectivity...

Snowflake DataIntegration :
--> Following ELT process, 

Dataintegration include  --> Data Preparation, Migration, Movement, managment,., data movement automation...

???
Load data from AWS S3 to Snoflake using push down optimization ;

https://quickstarts.snowflake.com/guide/harness_the_power_of_snowflake_with_informatica_idmc/#7

???
Semistructured JSON data to Snowflake :
https://quickstarts.snowflake.com/guide/harness_the_power_of_snowflake_with_informatica_idmc/#9



---- Userole, Usewarehouse, UseSchema ----- 

-- Set the correct ROLE, WAREHOUSE, and SCHEMA
use role PC_INFORMATICA_ROLE;
use warehouse PC_INFORMATICA_WH;
use schema PC_INFORMATICA_DB.PUBLIC;

-- JSONData ,,, VARIENT Datatype for JSON stage data access..
-- Createtable
create or replace table pc_informatica_db.public.daily_14_total (
  v variant,
  t timestamp);

-- Define a stage that describes where the data will be loaded from
-- CreateStage
create or replace stage weather_data_s3
  url = 's3://sfquickstarts/VHOL Informatica Data Management/WEATHER/';

-- Re-size the warehouse so we can load the data quicker
-- Alterwarehouse
alter warehouse pc_informatica_wh set warehouse_size = large;

--CopyDatafromstage
-- Load the data
copy into daily_14_total
   from (select $1, to_timestamp($1:time)
   from @weather_data_s3)
   file_format = (type=json);
   
-- Set the warehouse back to the original size
--AlterWarehouse...
alter warehouse pc_informatica_wh set warehouse_size = xsmall;


-- JSONHierarchial schema in IICS...
-- IICS has Hierarchial schema component .....

-- JSON Data,, JSON file load to snowflake, Source connection to snowflake to access JSON, bring source data to hierarchial parsor,, inner joins,, 
   another inner join to join the depedency data and load into snowflake single target....
-- Mapping task to schedule it and load...

************************************************
SQL development and data querying interfaces:
************************************************
Snowsight Worksheets  - Browser based editing & development...
SnowSQL   - Python-based client for performing all tasks in Snowflake, including querying, 
	    executing DDL/DML commands, and bulk loading/unloading of data.
Snowflake Extension for Visual Studio Code

.net, JDBC, ODBC, Pyhton drivers can be connected to snowflake for building...    --> Githib connectivity to python schema for deployments and all...

************************************************
Signing in to Snowsight  : UI
Working with Snowflake
- account identifier or account URL  (<orgname>.<account_name> )
- orgname
- Account_name
- cloud_region_id ( Region where snowflake is hosted , AWS)
- nowflake credentials-

Switch Role :  active role in your current session.Access of DB, Tables Access, Objects access, actions can be performed..

ACCOUNTADMIN
SYSADMIN
PUBLIC

Navigation :
Create -> SQL Worksheet, Python Worksheet, Streamlit App, Dashboard, Table, Stage, or View. 
Projects -> Worksheets, Notebooks, Streamlit, Dashboards, App Packages
Data -> Databases, and Add Data.
Data Products -> Marketplace, Apps, Private Sharing, Provider Studio.
Monitoring -> Query History, Copy History, Task History, Dynamic Tables, Trust Center, and Governance.

************************************************
Working with Snowflake
************************************************
Load a file into an existing table
Load a file into a new table while inferring the schema for the file.
Upload files from a stage into a table
View files in a stage.,,, Creating an S3 stage
Add & edit named stages.
See task graphs and run history., Viewing tasks and task graphs in Snowsight
Debug and rerun task graphs..Viewing tasks and task graphs in Snowsight
Create a file format ,, Creating a named file format


Dashboards, Worksheets, Databases, Schemas,Tables, Views, User-defined functions (UDF) and stored procedures.


Notebook : TO store notes..

GRANT USAGE ON DATABASE notebooks TO ROLE create_notebooks;
GRANT USAGE ON SCHEMA notebooks TO ROLE create_notebooks;
GRANT CREATE NOTEBOOK ON SCHEMA notebooks TO ROLE create_notebooks;

USE WAREHOUSE notebooks;

session.sql('USE WAREHOUSE notebooks;')

--> We can sync notebook to Git repository...

*******************************************
Accessing the session context for a notebook¶

from snowflake.snowpark.context import get_active_session
session = get_active_session()

SELECT CURRENT_WAREHOUSE(), CURRENT_DATABASE(), CURRENT_SCHEMA();

******************************************
Visualize results by using Altair¶ 

import altair as alt
alt.Chart(df).mark_bar().encode(
    x= alt.X("measurement", axis = alt.Axis(labelAngle=0)),
    y="value",
    color="species"
)
******************
import matplotlib.pyplot as plt
******************
pivot_df = pd.pivot_table(data=df, index=['measurement'], columns=['species'], values='value')
******************
import matplotlib.pyplot as plt
ax = pivot_df.plot.bar(stacked=True)
ax.set_xticklabels(list(pivot_df.index), rotation=0)
******************
import seaborn as sns

sns.barplot(
    data=df,
    x="measurement", hue="species", y="value",
)
******************
import streamlit as st
st.bar_chart(df, x='measurement', y='value', color='species')
******************


************************************
Sync Git Repository 
************************************
sync your notebook development with a branch in a Git repository.


************************************
Charts
************************************
Bar charts
Line charts
Scatterplots
Heat grids
Scorecards


************************************
Virtual warehouses
************************************
Warehouse --> Computing Power..
DML operations, including loading data into tables.
Warehouses can be started and stopped at any time.
compute resources available per cluster in a warehouse.

X-Small 1 0.0003
Small	2 0.0006
Medium 	4 0.0011
Large 	8 0.0022
X-Large 16
2X-Large 32

Impact on data loading
Increasing the size of a warehouse does not always improve data loading performance. 
Data loading performance is influenced more by the number of files being loaded (and the size of each file) than the size of the warehouse.

Tip

Unless you are bulk loading a large number of files concurrently (i.e. hundreds or thousands of files),
a smaller warehouse (Small, Medium, Large) is generally sufficient. 
Using a larger warehouse (X-Large, 2X-Large, etc.) will consume more credits and may not result in any performance increase.

complex queries -> more compute resources

Auto-suspension and auto-resumption :
By default, auto-suspend is enabled. Snowflake automatically suspends the warehouse if it is inactive for the specified period of time.

Query processing and concurrency
The number of queries that a warehouse can concurrently process is determined by the size and complexity of each query.
As queries are submitted, the warehouse calculates and reserves the compute resources needed to process each query. 

Default warehousing size will be created through create and alter...

****************************************************
What is a multi-cluster warehouse?¶
With multi-cluster warehouses, Snowflake supports allocating, either statically or dynamically, additional clusters
to make a larger pool of compute resources available. A multi-cluster warehouse is defined by specifying the following properties:

Maximum number of clusters, greater than 1 (up to 10).

****************************************************
Understanding Snowflake Table Structures
****************************************************


CreateTable :
*****************
CREATE TABLE hospital_table (patient_id INTEGER,
                             patient_name VARCHAR, 
                             billing_address VARCHAR,
                             diagnosis VARCHAR, 
                             treatment VARCHAR,
                             cost NUMBER(10,2));
INSERT INTO hospital_table 
        (patient_ID, patient_name, billing_address, diagnosis, treatment, cost) 
    VALUES
        (1, 'Mark Knopfler', '1982 Telegraph Road', 'Industrial Disease', 
            'a week of peace and quiet', 2000.00),
        (2, 'Guido van Rossum', '37 Florida St.', 'python bite', 'anti-venom', 
            70000.00)
        ;

CreateView :
****************
CREATE VIEW doctor_view AS
    SELECT patient_ID, patient_name, diagnosis, treatment FROM hospital_table;

CREATE VIEW accountant_view AS
    SELECT patient_ID, patient_name, billing_address, cost FROM hospital_table;

******************

CREATE VIEW v1 AS SELECT ... FROM my_database.my_schema.my_table;

CREATE VIEW v1 AS SELECT ... FROM my_schema.my_table;

CREATE VIEW v1 AS SELECT ... FROM my_table;

CREATE VIEW employee_hierarchy (title, employee_ID, manager_ID, "MGR_EMP_ID (SHOULD BE SAME)", "MGR TITLE") AS (
   WITH RECURSIVE employee_hierarchy_cte (title, employee_ID, manager_ID, "MGR_EMP_ID (SHOULD BE SAME)", "MGR TITLE") AS (
      -- Start at the top of the hierarchy ...
      SELECT title, employee_ID, manager_ID, NULL AS "MGR_EMP_ID (SHOULD BE SAME)", 'President' AS "MGR TITLE"
        FROM employees
        WHERE title = 'President'
      UNION ALL
      -- ... and work our way down one level at a time.
      SELECT employees.title, 
             employees.employee_ID, 
             employees.manager_ID, 
             employee_hierarchy_cte.employee_id AS "MGR_EMP_ID (SHOULD BE SAME)", 
             employee_hierarchy_cte.title AS "MGR TITLE"
        FROM employees INNER JOIN employee_hierarchy_cte
       WHERE employee_hierarchy_cte.employee_ID = employees.manager_ID
   )
   SELECT * 
      FROM employee_hierarchy_cte
);

********************
Limitations on Views :

1. The definition for a view cannot be updated.(Cannot use ALTER VIEW or ALTER MATERIALIZED VIEW to change the definition of a view)
2. Views are read-only (i.e. you cannot execute DML commands directly on a view). 


**********************************************************
Load Data into Snowflake
**********************************************************
1.Overview of data loading
2. Summary of data loading features
3. Data loading considerations (bulk data loading.)
4. Working With Amazon S3-compatible Storage (accessing data in other storage.)
5. Loading data using the web interface (mited amounts of data using the web interface)
6. Introduction to Loading Semi-structured Data.
7. Introduction to unstructured data
8.Bulk loading from a local file system ( loading data in bulk using the COPY command.)
9. Snowpipe ( loading data continuously using Snowpipe.)
10. Snowpipe Streaming ( loading data streams continuously using Snowpipe Streaming.)
11. Transforming data during a load (s for transforming data while loading it into a table using the COPY INTO command.)
12. Querying Data in Staged Files (query internal and external named stages.)


Bulk Loading , Continueous Load - SnowPipe..(- COPY INTO Command from Stage/internal object..)

Internal Stage : Files loaded into tables...stage type is designed to store files that are staged 
Table stages cannot be altered or dropped.,, 

***********BULKLOADING***************************
Bulk loading using the COPY command
This option enables loading batches of data from files already available in cloud storage, or copying (i.e. staging) data files from a 
local machine to an internal (i.e. Snowflake) cloud storage location before loading the data into tables using the COPY command.

*  While bulk loading we can increase the warehouse size, can upgrade or degrade.

********Bulk load transformation ********
Simple transformations during a load During Bulk Load COPY command :

Column reordering
Column omission
Casts
Truncating text strings that exceed the target column length..................


**************************************************
SNOWPIPE  ( Microbatches... load as soon as arrives)
****************************************************
Continuous loading using Snowpipe
This option is designed to load small volumes of data (i.e. micro-batches) and incrementally make them available for analysis.
Snowpipe loads data within minutes after files are added to a stage and submitted for ingestion.
This ensures users have the latest results, as soon as the raw data is available.

--> Coumpute resource has resources are automatically resized and scaled up or down as required,,,,

COPY Statements 
--> Bulk Loading Data...
--> Snowpipe  : to continously load micro-batches of data into staging tables for transformation 
		and optimization using automated tasks and the 
		change data capture (CDC) information : --> Using Streams....





	






